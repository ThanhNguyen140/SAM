{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code from [here](https://blog.roboflow.com/how-to-use-segment-anything-model-sam/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2445M  100 2445M    0     0  11.9M      0  0:03:25  0:03:25 --:--:-- 12.1M0     0  11.8M      0  0:03:27  0:00:18  0:03:09 12.1MM    0     0  11.8M      0  0:03:26  0:00:21  0:03:05 12.1M    0  11.9M      0  0:03:25  0:00:31  0:02:54 12.0M5  0:00:43  0:02:42 12.0M     0  11.9M      0  0:03:24  0:01:08  0:02:16 12.1M    0     0  11.9M      0  0:03:25  0:01:22  0:02:03 12.1M39M    0     0  11.9M      0  0:03:25  0:01:27  0:01:58 11.6M1.9M      0  0:03:25  0:01:34  0:01:51 12.0M     0  0:03:25  0:01:50  0:01:35 12.1M646M    0     0  11.9M      0  0:03:25  0:02:18  0:01:07 12.1M   0  0:03:25  0:02:36  0:00:49 12.1M0  11.9M      0  0:03:25  0:03:09  0:00:16 11.9M1.9M      0  0:03:25  0:03:12  0:00:13 12.1M\n",
      "Collecting git+https://github.com/facebookresearch/segment-anything.git\n",
      "  Cloning https://github.com/facebookresearch/segment-anything.git to /private/var/folders/tt/plrj7c_x5c769tsqdqgf30w40000gn/T/pip-req-build-f4t15s55\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/facebookresearch/segment-anything.git /private/var/folders/tt/plrj7c_x5c769tsqdqgf30w40000gn/T/pip-req-build-f4t15s55\n",
      "  Resolved https://github.com/facebookresearch/segment-anything.git to commit 6fdee8f2727f4506cfbbe553e23b895e27956588\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sam(\n",
       "  (image_encoder): ImageEncoderViT(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 1280, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0-31): 32 x Block(\n",
       "        (norm1): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): Attention(\n",
       "          (qkv): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((1280,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (lin1): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (lin2): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (neck): Sequential(\n",
       "      (0): Conv2d(1280, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (1): LayerNorm2d()\n",
       "      (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (3): LayerNorm2d()\n",
       "    )\n",
       "  )\n",
       "  (prompt_encoder): PromptEncoder(\n",
       "    (pe_layer): PositionEmbeddingRandom()\n",
       "    (point_embeddings): ModuleList(\n",
       "      (0-3): 4 x Embedding(1, 256)\n",
       "    )\n",
       "    (not_a_point_embed): Embedding(1, 256)\n",
       "    (mask_downscaling): Sequential(\n",
       "      (0): Conv2d(1, 4, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Conv2d(4, 16, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): LayerNorm2d()\n",
       "      (5): GELU(approximate='none')\n",
       "      (6): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (no_mask_embed): Embedding(1, 256)\n",
       "  )\n",
       "  (mask_decoder): MaskDecoder(\n",
       "    (transformer): TwoWayTransformer(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x TwoWayAttentionBlock(\n",
       "          (self_attn): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_token_to_image): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): MLPBlock(\n",
       "            (lin1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (lin2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (act): ReLU()\n",
       "          )\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm4): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (cross_attn_image_to_token): Attention(\n",
       "            (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "            (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (final_attn_token_to_image): Attention(\n",
       "        (q_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (k_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (v_proj): Linear(in_features=256, out_features=128, bias=True)\n",
       "        (out_proj): Linear(in_features=128, out_features=256, bias=True)\n",
       "      )\n",
       "      (norm_final_attn): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (iou_token): Embedding(1, 256)\n",
       "    (mask_tokens): Embedding(4, 256)\n",
       "    (output_upscaling): Sequential(\n",
       "      (0): ConvTranspose2d(256, 64, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (1): LayerNorm2d()\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): ConvTranspose2d(64, 32, kernel_size=(2, 2), stride=(2, 2))\n",
       "      (4): GELU(approximate='none')\n",
       "    )\n",
       "    (output_hypernetworks_mlps): ModuleList(\n",
       "      (0-3): 4 x MLP(\n",
       "        (layers): ModuleList(\n",
       "          (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "          (2): Linear(in_features=256, out_features=32, bias=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (iou_prediction_head): MLP(\n",
       "      (layers): ModuleList(\n",
       "        (0-1): 2 x Linear(in_features=256, out_features=256, bias=True)\n",
       "        (2): Linear(in_features=256, out_features=4, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load the model\n",
    "import torch\n",
    "from segment_anything import sam_model_registry\n",
    "!curl -O https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth #for Linux: wget {Link}\n",
    "!pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
    "!pip install -q roboflow supervision \n",
    "\n",
    "\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "MODEL_TYPE = \"vit_h\"\n",
    "CHECKPOINT_PATH = \"sam_vit_h_4b8939.pth\"\n",
    "sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)\n",
    "sam.to(device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image path here\n",
    "IMAGE_PATH = '/Users/lisa/Documents/Master/sam-lab/test_image.jpeg'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/lisa/Documents/Master/sam-lab/first_code_snippets.ipynb Cell 5\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lisa/Documents/Master/sam-lab/first_code_snippets.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m mask_generator \u001b[39m=\u001b[39m SamAutomaticMaskGenerator(sam)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lisa/Documents/Master/sam-lab/first_code_snippets.ipynb#W3sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m image_bgr \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(IMAGE_PATH)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lisa/Documents/Master/sam-lab/first_code_snippets.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m image_rgb \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39;49mcvtColor(image_bgr, cv2\u001b[39m.\u001b[39;49mCOLOR_BGR2RGB)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lisa/Documents/Master/sam-lab/first_code_snippets.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m result \u001b[39m=\u001b[39m mask_generator\u001b[39m.\u001b[39mgenerate(image_rgb)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# Automated Mask (Instance Segmentation) Generation with SAM\n",
    "import cv2\n",
    "from segment_anything import SamAutomaticMaskGenerator\n",
    "\n",
    "mask_generator = SamAutomaticMaskGenerator(sam)\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "result = mask_generator.generate(image_rgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/highgui/src/window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m/Users/lisa/Documents/Master/sam-lab/first_code_snippets.ipynb Cell 6\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lisa/Documents/Master/sam-lab/first_code_snippets.ipynb#X10sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cv2\u001b[39m.\u001b[39;49mimshow(\u001b[39m\"\u001b[39;49m\u001b[39mImage\u001b[39;49m\u001b[39m\"\u001b[39;49m, image_bgr)\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.8.0) /Users/xperience/GHA-OpenCV-Python/_work/opencv-python/opencv-python/opencv/modules/highgui/src/window.cpp:971: error: (-215:Assertion failed) size.width>0 && size.height>0 in function 'imshow'\n"
     ]
    }
   ],
   "source": [
    "cv2.imshow(\"Image\", image_bgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Annotate Masks with Supervision\n",
    "import supervision as sv\n",
    "\n",
    "mask_annotator = sv.MaskAnnotator(color_map = \"index\")\n",
    "detections = sv.Detections.from_sam(result)\n",
    "annotated_image = mask_annotator.annotate(image_bgr, detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate Segmentation Mask with Bounding Box\n",
    "\n",
    "import cv2\n",
    "from segment_anything import SamPredictor\n",
    "\n",
    "mask_predictor = SamPredictor(sam)\n",
    "\n",
    "image_bgr = cv2.imread(IMAGE_PATH)\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "mask_predictor.set_image(image_rgb)\n",
    "\n",
    "box = np.array([70, 247, 626, 926])\n",
    "masks, scores, logits = mask_predictor.predict(\n",
    "    box=box,\n",
    "    multimask_output=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
