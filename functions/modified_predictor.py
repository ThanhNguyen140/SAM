from typing import Optional, Tuple
from segment_anything.utils.transforms import ResizeLongestSide
import torch
from segment_anything import sam_model_registry

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
MODEL_TYPE = "vit_h"
CHECKPOINT_PATH = "../sam_vit_h_4b8939.pth"
sam = sam_model_registry[MODEL_TYPE](checkpoint=CHECKPOINT_PATH)
sam.to(device=DEVICE)


class modifiedPredictor:
    """Subclass of SamPredictor class. This class allows the generation of masks with the same syntax as the parent class for predict function
    using image embeddings
    """

    def __init__(self, model = sam):
        """Input for subclass of SamPredictor

        Args:
            image_embedding (NDArray): An image embedding generated by the parent class by set_image
            input_size (tuple): a tuple containing size of image before generating embeddings. This is not
            the orginal size of image because the image is resized according to sam model before generating embeddings
            original_size (tuple): original size of the image in 2D (H x W)
            sam_model (sam model, optional): Defaults to sam.
        """    
        self.model = model
        self.transform = ResizeLongestSide(self.model.image_encoder.img_size)
        self.input_size = (1024, 864)
        self.original_size = (256,216)

    @torch.no_grad()
    def predict(self,image_embeddings, point_coords,point_labels, multimask_output=False):
        """Predict mask logits by inputting image_embeddings, point coords and point_labels

        Args:
            image_embeddings (torch.tensor): tensor of (1 x 256 x 64 x 64) embeddings of one image
            point_coords (torch.tensor): tensor of (B x N x 2) point coords from the original images 
            point_labels (torch.tensor): tensor of (B x N) labels for point coords (0 for background and 1 for foreground)
            multimask_output (bool, optional): Different masks per image and point coords combination. Defaults to False.

        Returns:
            masks (torch.tensor): (B x H x W) mask logits 
        """

        point_coords = self.transform.apply_coords_torch(point_coords, self.original_size)

        points = (point_coords,point_labels)

        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(
                points=points,
                boxes= None,
                masks=  None
            )
        
        low_res_masks, iou_predictions = self.model.mask_decoder(
                image_embeddings.unsqueeze(0),
                image_pe=self.model.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                dense_prompt_embeddings=dense_embeddings,
                multimask_output=multimask_output,
            )
        masks = self.model.postprocess_masks(
            low_res_masks,
            input_size= self.input_size,
            original_size= self.original_size,
        )

        masks_cpu = masks.detach()
        point_coords.cpu()
        point_labels.cpu()
        return masks_cpu
